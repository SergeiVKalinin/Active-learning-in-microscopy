{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/utkarshp1161/Active-learning-in-microscopy/blob/main/notebooks/MultiFidelity(MultiTask)_GPyTorch.ipynb)\n",
    "# MultiFidelity(MultiTask) GP tutorial using GPyTorch \n",
    "\n",
    "This notebook demonstrates implementation of multitask GP in Gpytorch\n",
    "\n",
    "- Rewritten in Gpytorch by [Utkarsh Pratiush](https://github.com/utkarshp1161). Inspired from [Original implementation in Gpax](https://github.com/SergeiVKalinin/BO_Research/blob/master/MultiTaskGP_tutorial.ipynb) by [Maxim Ziatdinov](https://github.com/ziatdinovmax) and [SVK](https://github.com/SergeiVKalinin).\n",
    "- For reading related to this notebook please refer to paper by [Edwin et al 2007](https://www.google.com/search?q=Multi-task+Gaussian+Process+Prediction&oq=Multi-task+Gaussian+Process+Prediction&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQRRg8MgYIAhBFGD0yBggDEEUYPTIGCAQQLhhA0gEHMjU0ajBqMagCALACAA&sourceid=chrome&ie=UTF-8)\n",
    "- Reference to relevant [Gpytorch example](https://github.com/cornellius-gp/gpytorch/blob/main/examples/03_Multitask_Exact_GPs/Multitask_GP_Regression.ipynb)\n",
    "\n",
    "Warning: Though Multitask and Multifidelity are fundamentally different methods we are using the term interchangeably here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1a. Install modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install\n",
    "!pip install -q botorch==0.12.0\n",
    "!pip install -q gpytorch==1.13\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1b. Ground truth function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create toy datasets\n",
    "def theoretical_signal(x):\n",
    "    return 2 * np.sin(x/10) + 0.5 * np.sin(x/2) + 0.1 * x\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Fidelity 1 - \"theoretical model\"\n",
    "X1 = np.linspace(0, 100, 100)\n",
    "y1 = theoretical_signal(X1)\n",
    "\n",
    "# Fidelity 2 - \"experimental measurements\"\n",
    "X2 = np.concatenate([np.linspace(0, 25, 25), np.linspace(75, 100, 25)])\n",
    "y2 = 1.5 * theoretical_signal(X2) - 5 + np.random.normal(0, 0.5, X2.shape) + np.sin(X2/15)\n",
    "\n",
    "# Ground truth for Fidelity 2\n",
    "X_full_range = np.linspace(0, 100, 200)\n",
    "y2_true = 1.5 * theoretical_signal(X_full_range) - 5 + np.sin(X_full_range/15)\n",
    "\n",
    "# Plot the initial data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(X1, y1, 'b-', label='Theoretical Model (Fidelity 1)', alpha=0.6)\n",
    "plt.scatter(X2, y2, c='k', label='Experimental Data (Fidelity 2)', alpha=0.6)\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True function (Fidelity 2)', linewidth=2)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Signal Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('Initial Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1c Regular GP for experimental data alone:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. i) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. ii) sample data and set optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X2 and y2 to PyTorch tensors\n",
    "train_x = torch.from_numpy(X2).float()\n",
    "train_y = torch.from_numpy(y2).float()\n",
    "\n",
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, train_y, likelihood)\n",
    "\n",
    "\n",
    "# Training\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. iii) Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(training_iterations):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1c. iv) Posterior plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Create test points\n",
    "test_x = torch.linspace(0, 100, 200)\n",
    "\n",
    "# Get posterior distribution\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    mean = observed_pred.mean\n",
    "    lower, upper = observed_pred.confidence_region()\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "test_x_np = test_x.numpy()\n",
    "mean_np = mean.numpy()\n",
    "lower_np = lower.numpy()\n",
    "upper_np = upper.numpy()\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X2, y2, c='k', label='Training Data (Fidelity 2)', alpha=0.6)\n",
    "plt.plot(X1, y1, 'b-', label='Theoretical Model (Fidelity 1)', alpha=0.4)\n",
    "plt.plot(X_full_range, y2_true, 'k--', label='True Function (Fidelity 2)', alpha=0.6)\n",
    "plt.plot(test_x_np, mean_np, 'r-', label='Posterior Mean')\n",
    "plt.fill_between(test_x_np, lower_np, upper_np, alpha=0.3, color='red')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Signal Strength')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.title('GP Posterior Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1d. MultitaskGP "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. i) Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFidelityGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_latent):\n",
    "        super(MultiFidelityGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        \n",
    "        # Mean module\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        \n",
    "        # Base kernel for the data\n",
    "        self.base_covar = gpytorch.kernels.MaternKernel(ard_num_dims=1)\n",
    "        \n",
    "        # Kernel for fidelity interactions with configurable rank\n",
    "        self.task_covar = gpytorch.kernels.IndexKernel(num_tasks=2, rank=num_latent)\n",
    "        \n",
    "        # Scale the output\n",
    "        self.scaling = gpytorch.kernels.ScaleKernel(self.base_covar)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Split input into coordinates and task indicators\n",
    "        coordinates = x[..., 0:1]\n",
    "        task_indicators = x[..., 1].long()\n",
    "        \n",
    "        mean_x = self.mean_module(coordinates)\n",
    "        covar_x = self.scaling(coordinates)\n",
    "        covar_i = self.task_covar(task_indicators)\n",
    "        covar = covar_x.mul(covar_i)\n",
    "        \n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. ii) Define the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(num_latent, training_iterations=50):\n",
    "    # Initialize likelihood and model\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = MultiFidelityGPModel(train_x, train_y, likelihood, num_latent)\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(train_x)\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print('Iter %d/%d - Loss: %.3f' % (i + 1, training_iterations, loss.item()))\n",
    "        optimizer.step()\n",
    "\n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Create test points for fidelity 2\n",
    "    test_x = torch.from_numpy(np.column_stack((X_full_range, np.ones_like(X_full_range)))).float()\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        predictions = likelihood(model(test_x))\n",
    "        mean = predictions.mean\n",
    "        lower, upper = predictions.confidence_region()\n",
    "\n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(X_full_range, mean.numpy(), 'r-', label='Multi-fidelity GP prediction')\n",
    "    plt.fill_between(X_full_range, lower.numpy(), upper.numpy(), alpha=0.3, color='red')\n",
    "    plt.plot(X_full_range, y2_true, 'k--', label='Ground Truth')\n",
    "    plt.plot(X1, y1, 'b-', label='Theoretical Model (Fidelity 1)', alpha=0.6)\n",
    "    plt.scatter(X2, y2, c='k', label='Experimental Data (Fidelity 2)', alpha=0.6)\n",
    "    plt.xlabel('Frequency')\n",
    "    plt.ylabel('Signal Strength')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.title(f'Multi-fidelity GP Prediction (num_latent={num_latent})')\n",
    "    plt.show()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. iii) Choose parameters and run experiment - FOR num_latents = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_with_fidelity = np.column_stack([X1, np.zeros_like(X1)])  # Fidelity 1\n",
    "X2_with_fidelity = np.column_stack([X2, np.ones_like(X2)])   # Fidelity 2\n",
    "\n",
    "# Concatenate both fidelities\n",
    "train_x = torch.from_numpy(np.vstack([X1_with_fidelity, X2_with_fidelity])).float()\n",
    "train_y = torch.from_numpy(np.concatenate([y1, y2])).float()\n",
    "\n",
    "# Example usage with different number of latent dimensions\n",
    "num_latent = 1  # You can change this value\n",
    "model, likelihood = train_and_predict(num_latent, training_iterations = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1d. iv) Choose parameters and run experiment - FOR num_latents = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_latent = 2  # You can change this value\n",
    "model, likelihood = train_and_predict(num_latent, training_iterations = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boactivemat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
